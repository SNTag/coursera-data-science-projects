---
title: "trial v0.1"
author: "SNTag"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    css: style.css
    number_sections: false
    toc: false
    toc_depth: 3
    fig_caption: true
    fig_width: 7
    fig_height: 4
    includes:
      in_header: my_header.tex
---

Random scripts put together. each heading is a script for something random that I think will help me later on.

This is a scrap space, things are often deleted/changed.

```{r global_options, include=FALSE}
knitr::opts_chunk$set(echo=FALSE, warning=FALSE, message=FALSE)
```

https://rpubs.com/SNTag/633304

# CURRENT PROBLEMS TO SOLVE

- have ngrams been produced properly?
- why is string predictor NOT working right?

# should use more of:

object.size(): this function reports the number of bytes that an R object occupies in memory

Rprof(): this function runs the profiler in R that can be used to determine where bottlenecks in your function may exist. The profr package (available on CRAN) provides some additional tools for visualizing and summarizing profiling data.

gc(): this function runs the garbage collector to retrieve unused RAM for R. In the process it tells you how much memory is currently being used by R.

## ideas
- Kneser-Ney smoothing


# experiments
## handling daat
### cleaning data
files were downloaded from [here](https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip)
swear words from [here](https://github.com/RobertJGabriel/Google-profanity-words) (already will be read in below)

```{bash}
wget --show-progress --directory-prefix="~/Documents/coursera-data-science-projects/language-analysis/data/" "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
unzip ~/Documents/coursera-data-science-projects/language-analysis/data/Coursera-SwiftKey.zip
```


```{R}
pacman::p_load(magrittr,
               tidyverse,
               tidytext,
               tm)
set.seed(42)
test.string <- "this is a random test string"


### functions
#' will read in a file
#' @param filepath where the file is on the computer
#' @param size default is 0. if changed, will read in only 'n' lines from the file.
readFile <- function(filepath, size=0) {
        conn <- file(filepath, "r")
        fulltext <- readLines(conn, encoding="ascii", skipNul = TRUE) # warning: skipNul is true here
        if (size > 0) fulltext <- fulltext %>% sample(size)
        close(conn)
        return(fulltext)
    }

#' cleans the twitter dataset of IDs, tags, punctuation, number strings
#' @param input a file that has been read in.
#' @return cleaned data
removeTwitter <- function(input){
  input <- gsub("@\\w+","",input) # Removes twitter IDs
  input <- gsub("#\\w+","",input) # Removes twitter tags
  input <- gsub("[[:punct:]]{1,}","",input) # Removes punctuation
  input <- gsub("[0-9]+","",input) # Removes number strings
  return (input)
}

#' turns file sets into corpus
createCorpus <- function(fulltext) {
#        fulltext <- readFile(filepath)
        vs <- tm::VectorSource(fulltext)
        tm::Corpus(vs, readerControl=list(tm::readPlain, language="en", load=TRUE))
}

#' cleans corpus by removing all remaining extras (puncutations, capitilizations, profanity, stopwords)
cleanCorpus <- function (inputCorpus) {
  inputCorpus <- tm_map(inputCorpus, content_transformer(tolower)) # upper to lower
  inputCorpus <- tm_map(inputCorpus, content_transformer(removePunctuation)) # rm punct. as seen by tm
  inputCorpus <- tm_map(inputCorpus, content_transformer(removeNumbers)) # rm numbers

  # remove profanity and stopwords
  inputCorpus <- tm_map(inputCorpus, removeWords, c(stopwords("english"), profanity))
  inputCorpus <- tm_map(inputCorpus, content_transformer(stemDocument)) # get stem words e.g. rm -ing
  inputCorpus <- tm_map(inputCorpus, content_transformer(stripWhitespace)) # remove empty spaces
  return (inputCorpus)
}


### read in files + cleaning + conversion to documentMatrix
n          <- 10000
profanity  <- readFile("https://raw.githubusercontent.com/RobertJGabriel/Google-profanity-words/master/list.txt")
us.blogs   <-
    readFile("./data/final/en_US/en_US.blogs.txt", n) %>%
    as_tibble %>%
    removeTwitter %>%
    createCorpus %>%
    cleanCorpus
#    tm::DocumentTermMatrix()
us.twitter <-
    readFile("./data/final/en_US/en_US.twitter.txt", n) %>%
    as_tibble %>%
    removeTwitter %>%
    createCorpus %>%
    cleanCorpus
#    tm::DocumentTermMatrix()
us.news    <-
    readFile("./data/final/en_US/en_US.news.txt", n) %>%
    as_tibble %>%
    removeTwitter %>%
    createCorpus %>%
    cleanCorpus
#    tm::DocumentTermMatrix() %>%


### create one single massive corpus
us.data <- c(us.blogs, us.twitter, us.news) %>% createCorpus
```

### generate n-grams

#### tm

```{R}
#text.corpus <- tm::tm_map(us.data, tm::PlainTextDocument)

pacman::p_load(RWeka)


### using RWeka
length <- 2 # how many words on either side of word of interest
length1 <- 1 + length * 2
ngramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = length, max = length1 ))

monogramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1 ))
bigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2 ))
trigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3 ))
quadgramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 4, max = 4 ))

text.corpus <- tm::tm_map(us.data, tm::PlainTextDocument)
dtm <- TermDocumentMatrix(us.data, control = list(tokenize = ngramTokenizer))

dtm1 <- TermDocumentMatrix(us.data, control = list(tokenize = monogramTokenizer))
dtm2 <- TermDocumentMatrix(us.data, control = list(tokenize = bigramTokenizer))
dtm3 <- TermDocumentMatrix(us.data, control = list(tokenize = trigramTokenizer))
dtm4 <- TermDocumentMatrix(us.data, control = list(tokenize = quadgramTokenizer))

inspect(dtm1)
inspect(dtm2)
inspect(dtm3)
inspect(dtm4)


bigram<-function(x) NGramTokenizer(x,Weka_control(min=2,max=2))
bigramtab<-TermDocumentMatrix(us.data,control=list(tokenize=bigram))
bigramcorpus<-findFreqTerms(bigramtab,lowfreq=80)
bigramcorpusnum<-rowSums(as.matrix(bigramtab[bigramcorpus,]))
bigramcorpustab<-data.frame(Word=names(bigramcorpusnum),frequency=bigramcorpusnum)
bigramcorpussort<-bigramcorpustab[order(-bigramcorpustab$frequency),]
```

#### TidyText

```{R}


#' crass function to remove some remaining antagonizers
forceClean  <- function(input) {
    input[-(stringi::stri_detect(c(input$term), fixed = "\"") %>% which),]
    input[-(stringi::stri_detect(c(input$term), fixed = "(") %>% which),]
    input[-(stringi::stri_detect(c(input$term), fixed = "Ã¸") %>% which),]
}

pacman::p_load(tidytext, magrittr)

tmpVar <-
    us.data %>%
    TermDocumentMatrix() %>%
    tidy %>%
    forceClean %>%
    dplyr::filter(!stringr::str_detect(term, "[^A-Za-z\\d]"))

### using tidytext
data_frame1 <-
    tmpVar %>%
    unnest_tokens(output = bigram, input = term, token = "ngrams", n = 1) %>%
    dplyr::select(c(bigram, count)) %>%
    dplyr::arrange(desc(count)) %>%
    as.data.frame##  %>%
    ## na.omit
names(data_frame1)  <- c("terms", "freq")
data_frame1 <- data_frame1[-(data_frame1$terms %>% is.na %>% which),]

data_frame2 <-
    tmpVar %>%
    unnest_tokens(bigram, term, token = "ngrams", n = 2) %>%
    dplyr::select(c(bigram, count)) %>%
    dplyr::arrange(desc(count)) %>%
    as.data.frame##  %>%
    ## na.omit
names(data_frame2)  <- c("terms", "freq")
data_frame2 <- data_frame2[-(data_frame2$terms %>% is.na %>% which),]

data_frame3 <-
    tmpVar %>%
    unnest_tokens(bigram, term, token = "ngrams", n = 3) %>%
    dplyr::select(c(bigram, count)) %>%
    dplyr::arrange(desc(count)) %>%
    as.data.frame##  %>%
    ## na.omit
names(data_frame3)  <- c("terms", "freq")
data_frame3 <- data_frame3[-(data_frame3$terms %>% is.na %>% which),]

data_frame4 <-
    tmpVar %>%
    unnest_tokens(bigram, term, token = "ngrams", n = 4) %>%
    dplyr::select(c(bigram, count)) %>%
    dplyr::arrange(desc(count)) %>%
    as.data.frame##  %>%
    ## na.omit
names(data_frame4)  <- c("terms", "freq") #
data_frame4 <- data_frame4[-(data_frame4$terms %>% is.na %>% which),]
```

```{R}
word <- '103'
part_ngrams <- dtm$dimnames$Terms[grep(word, dtm$dimnames$Terms)]
part_ngrams
```

```{R}
saveRDS(data_frame1, file = "./data_frame1.rds")
saveRDS(data_frame2, file = "./data_frame2.rds")
saveRDS(data_frame3, file = "./data_frame3.rds")
saveRDS(data_frame4, file = "./data_frame4.rds")
```



## some files to play with

```{R}
load("~/Documents/Mastering-Text-Mining-with-R/Chapter 5/data_chap5/data_frame1.RData")
load("~/Documents/Mastering-Text-Mining-with-R/Chapter 5/data_chap5/data_frame2.RData")
load("~/Documents/Mastering-Text-Mining-with-R/Chapter 5/data_chap5/data_frame3.RData")
load("~/Documents/Mastering-Text-Mining-with-R/Chapter 5/data_chap5/data_frame4.RData")

fDF1 %>% head
fDF2 %>% head
fDF3 %>% head
fDF4 %>% head
```

```{R}
data_frame1 <- fDF1
data_frame2 <- fDF2
data_frame3 <- fDF3
data_frame4 <- fDF4
```


## with tm

```{R}
pacman::p_load(tm,
               RWeka,
               magrittr)
```

## string guessing: something to study

```{R}
pacman::p_load(tm,
               stringr)
# Load the n-gram data
## load("./data/data_frame1.RData");
## load("./data/data_frame2.RData");
## load("./data/data_frame3.RData");
## load("./data/data_frame4.RData");

CleanInputString<- function(input_string) {
  # cleaning up data
input_string<- iconv(input_string, "latin1", "ASCII", sub=" ");
input_string<- gsub("[^[:alpha:][:space:][:punct:]]", "", input_string);
  # corpus
input_corpus<- VCorpus(VectorSource(input_string))
input_corpus<- tm_map(input_corpus, content_transformer(tolower))
input_corpus<- tm_map(input_corpus, removePunctuation)
input_corpus<- tm_map(input_corpus, removeNumbers)
input_corpus<- tm_map(input_corpus, stripWhitespace)
input_string<- as.character(input_corpus[[1]])
input_string<- gsub("(^[[:space:]]+|[[:space:]]+$)", "", input_string)
if (nchar(input_string) > 0) {
return(input_string);
  } else {
return("");
  }
}

Get_next_word<- function(input_string) {
                                        # Data cleansing using the function written earlier
    input_string<- CleanInputString(input_string);
                                        # extract the string length
    input_string<- unlist(strsplit(input_string, split=" "));
    input_stringLen<- length(input_string);
    next_word_present<- FALSE;
    term_next<- as.character(NULL);
                                        # Katz- backoffN-gram model
    if (input_stringLen>= 3 & !next_word_present) {
                                        # collate the terms
        input_string1 <-
            paste(input_string[(input_stringLen-2):input_stringLen],
                  collapse=" ");
                                        # take the subset of 4-gram data
        searchStr<- paste("^",input_string1, sep = "");
        data_frame4Temp <- data_frame4[grep (searchStr, data_frame4$terms), ];
        if ( length(data_frame4Temp[,1]) > 1 )
        {
            term_next<- data_frame4Temp[1:10,1];# select 10 matching terms
            next_word_present<- TRUE;
        }
        data_frame4Temp <- NULL;
    }

                                        # 2. lets go to n-1 gram
    if (input_stringLen>= 2 & !next_word_present) {
                                        # collate input terms
        input_string1 <-
            paste(input_string[(input_stringLen-1):input_stringLen],
                  collapse=" ");
        searchStr<- paste("^",input_string1, sep = "");
        data_frame3Temp <- data_frame3[grep (searchStr, data_frame3$terms), ];
        if ( length(data_frame3Temp[, 1]) > 1 )
        {
            term_next<- data_frame3Temp[1:10,1];
            next_word_present<- TRUE;
        }
        data_frame3Temp <- NULL;
    }

    if (input_stringLen>= 1 & !next_word_present) {
        input_string1 <- input_string[input_stringLen];
        searchStr<- paste("^",input_string1, sep = "");
        data_frame2Temp <- data_frame2[grep (searchStr, data_frame2$terms), ];
        if ( length(data_frame2Temp[, 1]) > 1 )
        {
            term_next<- data_frame2Temp[1:10,1];
            next_word_present<- TRUE;
        }
        data_frame2Temp <- NULL;
    }

    if (!next_word_present&input_stringLen> 0) {
        term_next<- data_frame1$terms[1];
    }

    word_nxt<- word(term_next, -1);
    if (input_stringLen> 0) {
        df<- data.frame(word_nxt);
        return(df);
    } else {
        word_nxt<- "";
        df<- data.frame(word_nxt);
        return(df);
    }
}

Get_next_word("The guy in front of me just bought a pound of bacon, a bouquet, and a case of")
```

# Vocab

**inverse document frequency (IDF)**
commonly used measure of a term's selective potential

**term frequency (TF)**
weight of a term's appearance in a document

**TF \* IDF**
combines above. enables comparisons using dot product

# notes

- euclidean distances poor on small documents

- cosine similarity: reduces error on small corpus from euclidean

- PlainTextDocument vs DocumentTermMatrix?

# MY STRING PREDICTOR

```{R}
pacman::p_load(tm,
               stringr)
cleaningString <- function(input.string) {
    input.string <- iconv(input.string, "latin1", "ASCII", sub=" ")
    input.string<- gsub("[^[:alpha:][:space:][:punct:]]", "", input.string)
                                        # corpus
    input.corpus<- VCorpus(VectorSource(input.string))
    input.corpus<- tm_map(input.corpus, content_transformer(tolower))
    input.corpus<- tm_map(input.corpus, removePunctuation)
    input.corpus<- tm_map(input.corpus, removeNumbers)
    input.corpus<- tm_map(input_corpus, stripWhitespace)
    input.string<- as.character(input.corpus[[1]])
    input.string<- gsub("(^[[:space:]]+|[[:space:]]+$)", "", input.string)
    if (nchar(input.string) > 0) {
        return(input.string);
    } else {
        return("");
    }
}

predictNextWord <- function(input.string)
```
