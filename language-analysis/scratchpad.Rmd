---
title: "trial v0.1"
author: "SNTag"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    css: style.css
    number_sections: false
    toc: false
    toc_depth: 3
    fig_caption: true
    fig_width: 7
    fig_height: 4
    includes:
      in_header: my_header.tex
---

Random scripts put together. each heading is a script for something random that I think will help me later on.

This is a scrap space, things are often deleted/changed.

```{r global_options, include=FALSE}
knitr::opts_chunk$set(echo=FALSE, warning=FALSE, message=FALSE)
```

https://rpubs.com/SNTag/633304

# CURRENT PROBLEMS TO SOLVE

- have ngrams been produced properly?
- why is string predictor NOT working right?

# should use more of:

object.size(): this function reports the number of bytes that an R object occupies in memory

Rprof(): this function runs the profiler in R that can be used to determine where bottlenecks in your function may exist. The profr package (available on CRAN) provides some additional tools for visualizing and summarizing profiling data.

gc(): this function runs the garbage collector to retrieve unused RAM for R. In the process it tells you how much memory is currently being used by R.

## ideas
- Kneser-Ney smoothing


# experiments
## handling daat
### Generating corpus
files were downloaded from [here](https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip)
swear words from [here](https://github.com/RobertJGabriel/Google-profanity-words) (already will be read in below)


```{bash}
wget --show-progress --directory-prefix="~/Documents/coursera-data-science-projects/language-analysis/data/" "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
unzip ~/Documents/coursera-data-science-projects/language-analysis/data/Coursera-SwiftKey.zip
```


```{R}
pacman::p_load(magrittr,
               tidyverse,
               tidytext,
               tm)
set.seed(42)
test.string <- "this is a random test string"


### functions
#' will read in a file
#' @param filepath where the file is on the computer
#' @param size default is 0. if changed, will read in only 'n' lines from the file.
readFile <- function(filepath, size=0) {
        conn <- file(filepath, "r")
        fulltext <- readLines(conn, encoding="ascii", skipNul = TRUE) # warning: skipNul is true here
        if (size > 0) fulltext <- fulltext %>% sample(size)
        close(conn)
        return(fulltext)
    }

#' cleans the twitter dataset of IDs, tags, punctuation, number strings
#' @param input a file that has been read in.
#' @return cleaned data
removeTwitter <- function(input){
  input <- gsub("@\\w+","",input) # Removes twitter IDs
  input <- gsub("#\\w+","",input) # Removes twitter tags
  input <- gsub("[[:punct:]]{1,}","",input) # Removes punctuation
  input <- gsub("[0-9]+","",input) # Removes number strings
  return (input)
}

#' turns file sets into corpus
createCorpus <- function(fulltext) {
#        fulltext <- readFile(filepath)
        vs <- tm::VectorSource(fulltext)
        tm::Corpus(vs, readerControl=list(tm::readPlain, language="en", load=TRUE))
}

#' DISCARDED. using tm_map for cleaning has proven to be slower than using tidytext.
#' cleans corpus by removing all remaining extras (puncutations, capitilizations, profanity, stopwords)
cleanCorpus <- function (inputCorpus) { inputCorpus <- tm_map(inputCorpus,
  content_transformer(tolower)) # upper to lower inputCorpus <- tm_map(inputCorpus,
  content_transformer(removePunctuation)) # rm punct. as seen by tm inputCorpus <-
  tm_map(inputCorpus, content_transformer(removeNumbers)) # rm numbers

  # remove profanity and stopwords
  inputCorpus <- tm_map(inputCorpus, removeWords, c(stopwords("english"), profanity))
  inputCorpus <- tm_map(inputCorpus, content_transformer(stemDocument)) # get stem words e.g. rm -ing
  inputCorpus <- tm_map(inputCorpus, content_transformer(stripWhitespace)) # remove empty spaces
  return (inputCorpus)
}


### read in files + cleaning + conversion to documentMatrix
n          <- 0
profanity  <- readFile("https://raw.githubusercontent.com/RobertJGabriel/Google-profanity-words/master/list.txt")
us.blogs   <-
    readFile("./data/final/en_US/en_US.blogs.txt", n) %>%
    as_tibble %>%
    removeTwitter
#    createCorpus %>%
#    cleanCorpus
#    tm::DocumentTermMatrix()
us.twitter <-
    readFile("./data/final/en_US/en_US.twitter.txt", n) %>%
    as_tibble %>%
    removeTwitter
#    createCorpus %>%
#    cleanCorpus
#    tm::DocumentTermMatrix()
us.news    <-
    readFile("./data/final/en_US/en_US.news.txt", n) %>%
    as_tibble %>%
    removeTwitter
#    createCorpus %>%
#    cleanCorpus
#    tm::DocumentTermMatrix() %>%
```

#### Generating main corpus

```{R}
### create one single massive corpus
if (exists("./us_data.rds")) {
    readRDS("./us_data.rds")
} else {
    gc()
    print("1")
    us.data <- c(us.blogs, us.twitter, us.news) %>%
        createCorpus %>%
        TermDocumentMatrix
    gc()
    print("2")
    us.data <- us.data %>%
        tidy
    gc()
    print("3")
    us.data <- us.data %>%
        unnest_tokens(word, term)
    gc()
    print("4")
    us.data <- us.data %>%
        anti_join(stop_words)
    gc()
    print("5")
    us.data <- us.data %>%
        dplyr::filter(!stringr::str_detect(word, "[^A-Za-z\\d]"))
    saveRDS(us.data, file = "./us_data.rds")
}
```

### generate n-grams

```{R}
#' DISCARDED. Use of tidy text is making this unnecessary.
#' crass function to remove some remaining antagonizers
forceClean  <- function(input) {
    input[-(stringi::stri_detect(c(input$word), fixed = "\"") %>% which),]
    input[-(stringi::stri_detect(c(input$word), fixed = "(") %>% which),]
    input[-(stringi::stri_detect(c(input$word), fixed = "Ã¸") %>% which),]
}

pacman::p_load(tidytext, magrittr)

### using tidytext
data_frame1 <-
    us.data %>%
    unnest_tokens(output = bigram, input = word, token = "ngrams", n = 1) %>%
    dplyr::select(c(bigram, count)) %>%
    dplyr::arrange(desc(count)) %>%
    as.data.frame
names(data_frame1)  <- c("terms", "freq")
#data_frame1 <- data_frame1[-(data_frame1$terms %>% is.na %>% which),]

data_frame2 <-
    us.data %>%
    unnest_tokens(bigram, word, token = "ngrams", n = 2) %>%
    dplyr::select(c(bigram, count)) %>%
    dplyr::arrange(desc(count)) %>%
    as.data.frame
names(data_frame2)  <- c("terms", "freq")
data_frame2 <- data_frame2[-(data_frame2$terms %>% is.na %>% which),]

data_frame3 <-
    us.data %>%
    unnest_tokens(bigram, word, token = "ngrams", n = 3) %>%
    dplyr::select(c(bigram, count)) %>%
    dplyr::arrange(desc(count)) %>%
    as.data.frame
names(data_frame3)  <- c("terms", "freq")
data_frame3 <- data_frame3[-(data_frame3$terms %>% is.na %>% which),]

data_frame4 <-
    us.data %>%
    unnest_tokens(bigram, word, token = "ngrams", n = 4) %>%
    dplyr::select(c(bigram, count)) %>%
    dplyr::arrange(desc(count)) %>%
    as.data.frame
names(data_frame4)  <- c("terms", "freq") #
data_frame4 <- data_frame4[-(data_frame4$terms %>% is.na %>% which),]
```

## some files to play with

```{R}
load("~/Documents/Mastering-Text-Mining-with-R/Chapter 5/data_chap5/data_frame1.RData")
load("~/Documents/Mastering-Text-Mining-with-R/Chapter 5/data_chap5/data_frame2.RData")
load("~/Documents/Mastering-Text-Mining-with-R/Chapter 5/data_chap5/data_frame3.RData")
load("~/Documents/Mastering-Text-Mining-with-R/Chapter 5/data_chap5/data_frame4.RData")

fDF1 %>% head
fDF2 %>% head
fDF3 %>% head
fDF4 %>% head
```

```{R}
data_frame1 <- fDF1
data_frame2 <- fDF2
data_frame3 <- fDF3
data_frame4 <- fDF4
```


## with tm

```{R}
pacman::p_load(tm,
               RWeka,
               magrittr)
```

## string guessing: something to study

```{R}
pacman::p_load(tm,
               stringr)
# Load the n-gram data
## load("./data/data_frame1.RData");
## load("./data/data_frame2.RData");
## load("./data/data_frame3.RData");
## load("./data/data_frame4.RData");

CleanInputString<- function(input_string) {
  # cleaning up data
input_string<- iconv(input_string, "latin1", "ASCII", sub=" ");
input_string<- gsub("[^[:alpha:][:space:][:punct:]]", "", input_string);
  # corpus
input_corpus<- VCorpus(VectorSource(input_string))
input_corpus<- tm_map(input_corpus, content_transformer(tolower))
input_corpus<- tm_map(input_corpus, removePunctuation)
input_corpus<- tm_map(input_corpus, removeNumbers)
input_corpus<- tm_map(input_corpus, stripWhitespace)
input_string<- as.character(input_corpus[[1]])
input_string<- gsub("(^[[:space:]]+|[[:space:]]+$)", "", input_string)
if (nchar(input_string) > 0) {
return(input_string);
  } else {
return("");
  }
}

Get_next_word<- function(input_string) {
                                        # Data cleansing using the function written earlier
    input_string<- CleanInputString(input_string);
                                        # extract the string length
    input_string<- unlist(strsplit(input_string, split=" "));
    input_stringLen<- length(input_string);
    next_word_present<- FALSE;
    term_next<- as.character(NULL);
                                        # Katz- backoffN-gram model
    if (input_stringLen>= 3 & !next_word_present) {
                                        # collate the terms
        input_string1 <-
            paste(input_string[(input_stringLen-2):input_stringLen],
                  collapse=" ");
                                        # take the subset of 4-gram data
        searchStr<- paste("^",input_string1, sep = "");
        data_frame4Temp <- data_frame4[grep (searchStr, data_frame4$terms), ];
        if ( length(data_frame4Temp[,1]) > 1 )
        {
            term_next<- data_frame4Temp[1:10,1];# select 10 matching terms
            next_word_present<- TRUE;
        }
        data_frame4Temp <- NULL;
    }

                                        # 2. lets go to n-1 gram
    if (input_stringLen>= 2 & !next_word_present) {
                                        # collate input terms
        input_string1 <-
            paste(input_string[(input_stringLen-1):input_stringLen],
                  collapse=" ");
        searchStr<- paste("^",input_string1, sep = "");
        data_frame3Temp <- data_frame3[grep (searchStr, data_frame3$terms), ];
        if ( length(data_frame3Temp[, 1]) > 1 )
        {
            term_next<- data_frame3Temp[1:10,1];
            next_word_present<- TRUE;
        }
        data_frame3Temp <- NULL;
    }

    if (input_stringLen>= 1 & !next_word_present) {
        input_string1 <- input_string[input_stringLen];
        searchStr<- paste("^",input_string1, sep = "");
        data_frame2Temp <- data_frame2[grep (searchStr, data_frame2$terms), ];
        if ( length(data_frame2Temp[, 1]) > 1 )
        {
            term_next<- data_frame2Temp[1:10,1];
            next_word_present<- TRUE;
        }
        data_frame2Temp <- NULL;
    }

    if (!next_word_present&input_stringLen> 0) {
        term_next<- data_frame1$terms[1];
    }

    word_nxt<- word(term_next, -1);
    if (input_stringLen> 0) {
        df<- data.frame(word_nxt);
        return(df);
    } else {
        word_nxt<- "";
        df<- data.frame(word_nxt);
        return(df);
    }
}

Get_next_word("The guy in front of me just bought a pound of bacon, a bouquet, and a case of")
Get_next_word("You're the reason why I smile everyday. Can you follow me please? It would mean the")
Get_next_word("Hey sunshine, can you follow me and make me the")
Get_next_word("Very early observations on the Bills game: Offense still struggling but the")
Get_next_word("Go on a romantic date at the")
Get_next_word("Well I'm pretty sure my granny has some old bagpipes in her garage I'll dust them off and be on my")
Get_next_word("Ohhhhh #PointBreak is on tomorrow. Love that film and haven't seen it in quite some")
Get_next_word("After the ice bucket challenge Louis will push his long wet hair out of his eyes with his little")
Get_next_word("Be grateful for the good times and keep the faith during the")
Get_next_word("If this isn't the cutest thing you've ever seen, then you must be")
```

# Vocab

**inverse document frequency (IDF)**
commonly used measure of a term's selective potential

**term frequency (TF)**
weight of a term's appearance in a document

**TF \* IDF**
combines above. enables comparisons using dot product

# notes

- euclidean distances poor on small documents

- cosine similarity: reduces error on small corpus from euclidean

- PlainTextDocument vs DocumentTermMatrix?

# MY STRING PREDICTOR

```{R}
pacman::p_load(tm,
               stringr)
cleaningString <- function(input.string) {
    input.string <- iconv(input.string, "latin1", "ASCII", sub=" ")
    input.string<- gsub("[^[:alpha:][:space:][:punct:]]", "", input.string)
                                        # corpus
    input.corpus<- VCorpus(VectorSource(input.string))
    input.corpus<- tm_map(input.corpus, content_transformer(tolower))
    input.corpus<- tm_map(input.corpus, removePunctuation)
    input.corpus<- tm_map(input.corpus, removeNumbers)
    input.corpus<- tm_map(input_corpus, stripWhitespace)
    input.string<- as.character(input.corpus[[1]])
    input.string<- gsub("(^[[:space:]]+|[[:space:]]+$)", "", input.string)
    if (nchar(input.string) > 0) {
        return(input.string);
    } else {
        return("");
    }
}

predictNextWord <- function(input.string)
```
