---
title: "Analysis on Exercise Data"
author: "SNTag"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    number_sections: true
    toc: false
    toc_depth: 3
    fig_caption: true
    fig_width: 7
    fig_height: 4
    includes:
      in_header: my_header.tex
---

```{R self_running, eval=FALSE, include = FALSE}
knitr::purl("./prediction-assignment-completecase.Rmd")
source("./prediction-assignment-completecase.R")
```

# Introduction

This project will attempt to identify exercise regimes followed from biological data. This will
give us a guide to what is necessary for proper development.

Goal: to identify the exercise regime identified in the classe variable.


```{R loading_data, echo = T, eval = T, warning = F, message = F}
pacman::p_load(tidyverse,
               magrittr,
               caret,
               doParallel)

train <- read_csv("./data/pml-training.csv")
test  <- read_csv("./data/pml-testing.csv")

train$classe <- train$classe %>% as.factor

set.seed("1701")
```

# Exploratory data analysis

The training data for this project has `r dim(train)[1]` entries, belonging
to `r train$user_name %>% unique %>% length`. We are interested in using this data to predict
how `r dim(test)[1]` entries should look.

In a quick test to check distribution of NAs, 2/3rds of the columns are nearly entirely NA, with
only some data. None are entirely NAs though.

```{R checking_for_NAs, echo = T, eval = T, fig.align = "center", fig.cap = "Percentage of rows per column that are NA."}
missing.values <- apply(train, 2, function(x) mean(is.na(x))) > 0.95
train          <- train[, -which(missing.values, label == FALSE)]
test           <- test[, -which(missing.values, label == FALSE)]


missing.values.count <- train %>%
    dplyr::summarize_all(funs(sum(is.na(.))/dim(train)[1]))
missing.values.count <-
    gather(missing.values.count, key="feature", value="missing_pct")
missing.values.count %>%
    ggplot(aes(x=reorder(feature,-missing_pct),y=missing_pct)) +
    geom_bar(stat="identity",fill="red")+
    coord_flip()+theme_bw()
```

We also need to handle near zero variances (NZV). Additionally, the first 5 columns are of useless information.

```{R handling_near_zero_var}
dim(train)
NZV   <- nearZeroVar(train)
train <- train[ ,-NZV]
test  <- test[ ,-NZV]

train <- train[ , -(1:5)]
test <- test[ , -(1:5)]
```

Further examination of the data shows that the same rows have NAs (Fig. 1). If one row has NAs, others will
too. I am unsure how to make the best use of this information so I will remove these NAs.

```{R removing_NAs, echo = T, eval = T}
missing.values.logic <- sapply(test, function(x) any(is.na(x)))
train.cc         <- train[,!missing.values.logic]
train.cc$classe   <- train$classe %>% as.factor
```


And finally, to test my models capabilities, I will split the training data set 80-20. The model
will be trained on the 80%, and tested on the 20%, before being applied to data in question
(test).

```{R data_split, echo = T, eval = T}
trainIndex <- caret::createDataPartition(train.cc$classe,
                                         p = .8,
                                         list = FALSE,
                                         times = 1)
train.cc80 <- train.cc[trainIndex,]
train.cc20 <- train.cc[-trainIndex,]
```


# Classification

We are interested in a model capable of predicting the classe. For this, we will compare between
random forest and gbm. Two robost algorithim. They will be used with the kappa metric. Finally,
cross-validation will be completed between the two models. Both models will be repeated with k-fold
10.

```{R classification_rf, echo = T, eval = T}
#if (exists("rf_fit")) {
#    rf.fit <- readRDS("fit.rds")
#} else {
    cl <- makePSOCKcluster(5)
    registerDoParallel(cl)

    fitControl <- caret::trainControl(method = "cv",
                                      number = 10)
    rf.fit <- caret::train(classe~.,
                           data=train.cc80,
                           method="rf",
                           metric="Kappa",
                           trControl=fitControl)

    stopCluster(cl)
#    saveRDS(rf.fit, file = "fit.rds")
#}
```
```{R classification_gbm, echo = T, eval = T}
#if (!exists("gbm.fit")) {
#    gbm.fit <- readRDS("gbm.rds")
#} else {
cl <- makePSOCKcluster(5)
    registerDoParallel(cl)

fitControl <- caret::trainControl(method = "cv",
                                      number = 10)

gbm.fit <- train(classe ~ .,
                 data = train.cc80,
                 method = "gbm",
                 metric = "Kappa",
                 trControl = fitControl,
                 verbose = FALSE)

    stopCluster(cl)
#    saveRDS(rf.fit, file = "gbm.rds")
#}
```

looking at the resulting models, we get:

```{R model_summary, echo = T, eval = T}
rf.fit
#gbm.fit

library(lattice)
resample.result <- caret::resamples(list(rf = rf.fit, gbm = gbm.fit))
summary(resample.result)
```

Interestingly, both models have a relatively high accuracy. This assignment will continue using the
random forest model. This approach will proceed. It also seems to have high sensitivity and
specificity when applied to the split data. Finally, error can be found using 'confusionMatrix'.

```{R rf_model_summary, echo = T, eval = T}
results <- c()
results$predicted <- predict(rf.fit,newdata = train.cc20)
results$classe <- train.cc20$classe
results <- as.data.frame(results)
print("number of incorrectly predicted rows:")
length(which(results$predicted != results$classe))

caret::confusionMatrix(train.cc20$classe, predict(rf.fit, newdata = train.cc20))
```


# Results

```{R final_model, echo = T, eval = T}
finalResults <- predict(rf.fit, newdata = test)
print(finalResults)
```

As the significance from the earlier steps is high, this is the solution.
